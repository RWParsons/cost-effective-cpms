---
title: "Analyses"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: rmarkdown::github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE)
set.seed(42)
library(fitdistrplus)
library(tidyverse)
library(data.table)
library(ggridges)
library(bayestestR)
library(cutpointr)
library(kableExtra)
library(formattable)
library(cowplot)
library(parallel)
source("src/inputs.R")
source("src/utils.R")
source("src/cutpoint_methods.R")
source("src/summary.R")

do_new_analyses <- FALSE
```


Question: What are the differences in NMB between models where the Probability threshold was based on the currently available methods versus costs-based selection. (Hospital falls as a use case.)

1. Define costs of a TP, TN, FP, FN of falls classification (option to move this into the loop where costs are sampled from a distributions to account for uncertainty in their estimates in the literature)
    * FP have cost of applying intervention
    * FN have cost of patient fall
    * TP have cost of intervention + cost of fall*(1-effectiveness of intervention on rate of falls)
    * TN are cost $0
2. Select appropriate ranges for model AUC (~0.75?) and prevalence (~3%) for comparable clinical prediction model for falls.
3. For sample sizes (N) in [100, 500, 1000]: (repeat 500 times at each sample size)
    * Get training data by sampling observed predictor values and outcome by transforming AUC into Cohens' D and sampling from two normal distributions, the first (negative events) with mean=0 and the second (positive events) with mean=Cohens'D. (Both with sd=1.)
    * Fit a logistic regression model using this sampled data.
    * Fit predicted probabilities to the training data and use these to obtain probability thresholds using each method.
    * Get validation data using the same approach but with n=1000.
    * Use the previously fit model to estimate probabilities for validation data.
    * Evaluate the thresholds selected using the training data on the validation data, in terms of mean cost per patient.
4. Measure differences in NMB on validation sample dependent on use of currently available methods and cost-based approach to determine threshold.
5. Observe whether this relationship is dependent on the sample size taken
6. ???


```{r}

get_nmb <- function(){
  # WTP from Edney et al (2018), Pharmacoeconomics
  WTP <- params$global$WTP
  
  # treatment_effect taken from: Haines et al. (2010) Archives of Internal Medicine
  treatment_effect <- exp(rnorm(
    1, 
    mean=params$falls$treatment_log_hazard$mean,
    sd=params$falls$treatment_log_hazard$sd
  ))
  
  # taken from abstract of Hill (2015), Lancet
  treatment_cost <- params$falls$treatment_cost
  
  # taken from Morello et al (2015). MJA
  
  falls_cost <- 
    rgamma(
      1, 
      params$falls$falls_cost$shape, 
      params$falls$falls_cost$rate
    ) * params$falls$falls_cost$multiplier
  
  #taken from Latimer et al (2013) Age and Ageing
  fall_eff <- rbeta(
    1,
    params$falls$fall_decrement$shape1,
    params$falls$fall_decrement$shape2
  ) * 0.5 #Latimer conducted 6-month follow-up <- 0.5*utility = QALY
    
  c(
    "TN"=0,
    "FN"=-falls_cost - fall_eff*WTP,
    "TP"=-falls_cost*(1-treatment_effect) - treatment_cost - fall_eff*WTP,
    "FP"=-treatment_cost
  )
}
get_nmb()

# the same as get_nmb for falls but returns only the point estimates.
get_nmb_est <- function() {
  WTP <- params$global$WTP
  
  # treatment_effect taken from: Haines et al. (2010) Archives of Internal Medicine
  treatment_effect <- exp(params$falls$treatment_log_hazard$mean)
  
  # taken from abstract of Haines (2010), BMC Medicine
  treatment_cost <- params$falls$treatment_cost
  
  # taken from Morello et al (2015). MJA
  
  falls_cost <- 6669 * params$falls$falls_cost$multiplier
  
  #taken from Latimer et al (2013) Age and Ageing
  fall_eff <- 0.04206168 * 0.5 #Latimer conducted 6-month follow-up <- 0.5*utility = QALY
    
  c(
    "TN"=0,
    "FN"=-falls_cost - fall_eff*WTP,
    "TP"=-falls_cost*(1-treatment_effect) - treatment_cost - fall_eff*WTP,
    "FP"=-treatment_cost
  )
}

```


```{r}
get_nmb_ICU <- function(){
  
  WTP <- params$global$WTP

  # Treatment effect taken from de Vos et al (2022), Value in Health
  eff_disch <- rbeta(
    1,
    params$icu$ward_eff$shape1,
    params$icu$ward_eff$shape2
  ) * params$icu$ward_eff$multiplier
  
  # ICU occupancy cost taken from Hicks et al (2019), MJA
  ICU_cost <- rgamma(
    1, 
    params$icu$icu_cost$shape,
    params$icu$icu_cost$rate
  ) * params$icu$icu_cost$multiplier
  
  # Opportunity cost taken from Page et al (2017), BMC HSR
  ICU_opp_cost <- params$icu$opp_cost
  
  # ICU readmission LOS taken from Chen et al (1998), Crit Care Med
  ICU_readmit <- rgamma(
    1, 
    params$icu$icu_readmit_los$shape,
    params$icu$icu_readmit_los$rate,
  )
  
  c(
    "TN"=eff_disch*WTP,
    "FN"=eff_disch*WTP - ICU_readmit*ICU_cost,
    "TP"=-ICU_cost,
    "FP"=-ICU_cost - ICU_opp_cost
  )
}
get_nmb_ICU()

# Repeat point estimate replacement for ICU
get_nmb_est_ICU <- function() {
  WTP <- params$global$WTP
  
  # Treatment effect taken from de Vos et al (2022), Value in Health
  eff_disch <- 0.42
  
  # ICU occupancy cost taken from Hicks et al (2019), MJA
  ICU_cost <- 4375*params$icu$icu_cost$multiplier
  
   # Opportunity cost taken from Page et al (2017), BMC HSR
  ICU_opp_cost <- params$icu$opp_cost
  
  # ICU readmission LOS taken from Chen et al (2021), World Journal of Surgery
  ICU_readmit <- 7.8
    
  c(
    "TN"=eff_disch*WTP,
    "FN"=eff_disch*WTP - unname(ICU_readmit)*ICU_cost,
    "TP"=-ICU_cost,
    "FP"=-ICU_cost - ICU_opp_cost
  )
}
```


### Run simulation
```{r}

do_simulation <- function(sample_size, n_sims, n_valid, sim_auc, event_rate, 
                          fx_costs_training, fx_costs_evaluation,
                          intercept_adjustment=0, return_calibration_plot=F,
                          seed=42, ...){
  
  if(!is.null(seed)) set.seed(seed)
  if(missing(sample_size)) {
    pmsamp <- pmsampsize::pmsampsize(
      type="b",
      cstatistic=sim_auc,
      parameters=1,
      prevalence=event_rate
    )
    sample_size <- pmsamp$sample_size
    min_events <- pmsamp$events
    check_events <- TRUE
  } else {
    check_events <- FALSE
  }
  if(return_calibration_plot){
    p_calibration <- ggplot() + geom_abline()
  }
  
  i <- 0
  while(i < n_sims){
    train_sample <- get_sample(auc=sim_auc, n_samples=sample_size, prevalence=event_rate)
    valid_sample <- get_sample(auc=sim_auc, n_samples=n_valid, prevalence=event_rate)
    if(length(unique(train_sample$actual))!=2 | length(unique(valid_sample$actual))!=2){
      next
    }
    if(check_events){
      if(sum(train_sample$actual) < min_events){
        next
      }
    }
    i <- i + 1
    model <- glm(actual~predicted, data=train_sample, family=binomial())
    
    if(intercept_adjustment!=0){
      train_log_odds <- predict(model) + intercept_adjustment
      train_sample$predicted <- exp(train_log_odds)/(1+exp(train_log_odds))
      
      valid_log_odds <- predict(model, newdata=valid_sample) + intercept_adjustment
      valid_sample$predicted <- exp(valid_log_odds)/(1+exp(valid_log_odds))
    } else {
      train_sample$predicted <- predict(model, type="response")
      valid_sample$predicted <- predict(model, type="response", newdata=valid_sample)
    }
    
    if(return_calibration_plot){
      df_calplot <- train_sample %>%
        mutate(bin = ntile(predicted, 10)) %>%
        group_by(bin) %>%
        summarize(
          n=n(), 
          bin_pred=mean(predicted),
          bin_prob=mean(actual),
          se=sqrt((bin_prob * (1 - bin_prob)) / n),
          ul = bin_prob + 1.96*se,
          ll = bin_prob - 1.96*se
        ) %>%
        ungroup() 
      
      # p_calibration <- p_calibration + geom_pointrange(data=df_calplot, aes(x=bin_pred, y=bin_prob, ymin=ll, ymax=ul), alpha=0.2)
      p_calibration <- p_calibration + geom_line(data=df_calplot, aes(x=bin_pred, y=bin_prob), alpha=0.2)
      p_calibration <- p_calibration + geom_point(data=df_calplot, aes(x=bin_pred, y=bin_prob), alpha=0.2)
    }
    
    
    training_value_vector <- fx_costs_training()
    
    thresholds <- get_thresholds(
      predicted=train_sample$predicted, 
      actual=train_sample$actual,
      costs=training_value_vector
    )
    
    evaluation_value_vector <- fx_costs_evaluation()
    
    cost_threshold <- function(pt){
      classify_samples(
        predicted=valid_sample$predicted,
        actual=valid_sample$actual,
        pt=pt,
        costs=evaluation_value_vector
      )
    }
    
    results_i <-
      unlist(thresholds) %>%
      map_dbl(cost_threshold) %>%
      t()
    thresholds_i <- unlist(thresholds)
    if(i==1){
      df_result <- results_i
      df_thresholds <- thresholds_i
    } else {
      df_result <- rbind(df_result, results_i)
      df_thresholds <- rbind(df_thresholds, thresholds_i)
    }
  } # end simulation loop
  
  df_result <- as.data.frame.matrix(df_result)
  df_thresholds <- as.data.frame.matrix(df_thresholds)
  rownames(df_thresholds) <- NULL
  
  df_result <- add_column(df_result, n_sim=1:nrow(df_result), .before=T)
  df_thresholds <- add_column(df_thresholds, n_sim=1:nrow(df_thresholds), .before=T)
  
  res <- list(
    date_time=Sys.time(),
    df_result=df_result,
    df_thresholds=df_thresholds,
    # calibration_plot=p_calibration,
    meta_data=list(
      sample_size=sample_size, 
      n_sims=n_sims, 
      n_valid=n_valid, 
      sim_auc=sim_auc, 
      event_rate=event_rate, 
      fx_costs_training=fx_costs_training,
      fx_costs_evaluation=fx_costs_evaluation
    )
  )
  
  if(return_calibration_plot){
     p_calibration <- 
       p_calibration +
       theme_bw() +
       labs(
         x="Predicted Probability",
         y="Observed Probability"
        )
     res <- append(res, list(calibration_plot = p_calibration))
  }
  
  res
}


# x <- do_simulation(
#   # sample_size=500,
#   n_sims=100, n_valid=1000, sim_auc=0.6, event_rate=0.01,
#   # fx_costs_training=get_nmb_est_ICU, fx_costs_evaluation=get_nmb_ICU,
#   fx_costs_training=get_nmb_est, fx_costs_evaluation=get_nmb,
#   plot_type = "histogram",
#   scale=1
# )


# x$df_result %>%
#   plot_fw_histogram(hdi=F, plot_labels=labs(x="Net Monetary Benefit (AUD)\n", y=""))

```


```{r}
cfg <- list(
  n_sim=500,
  n_valid=10000,
  sim_auc=0.7, 
  event_rate=0.036
)
if (do_new_analyses | !file.exists("output/calibration_experiment/calibration_simulations.rds")) {
  x_calibrated <- 
    do_simulation(
      n_sims=cfg$n_sim, n_valid=cfg$n_valid, sim_auc=cfg$sim_auc, event_rate=cfg$event_rate,
      fx_costs_training=get_nmb_est, fx_costs_evaluation=get_nmb,
      intercept_adjustment=0, return_calibration_plot = T
    )
  
  x_adjusted_up <- 
    do_simulation(
      n_sims=cfg$n_sim, n_valid=cfg$n_valid, sim_auc=cfg$sim_auc, event_rate=cfg$event_rate,
      fx_costs_training=get_nmb_est, fx_costs_evaluation=get_nmb,
      intercept_adjustment=1, return_calibration_plot = T
    )
  
  x_adjusted_down <- 
    do_simulation(
      n_sims=cfg$n_sim, n_valid=cfg$n_valid, sim_auc=cfg$sim_auc, event_rate=cfg$event_rate,
      fx_costs_training=get_nmb_est, fx_costs_evaluation=get_nmb,
      intercept_adjustment=-1, return_calibration_plot = T
    )
  
  x_adjusted_up2 <- 
    do_simulation(
      n_sims=cfg$n_sim, n_valid=cfg$n_valid, sim_auc=cfg$sim_auc, event_rate=cfg$event_rate,
      fx_costs_training=get_nmb_est, fx_costs_evaluation=get_nmb,
      intercept_adjustment=2, return_calibration_plot = T
    )
  
  x_adjusted_down2 <- 
    do_simulation(
      n_sims=cfg$n_sim, n_valid=cfg$n_valid, sim_auc=cfg$sim_auc, event_rate=cfg$event_rate,
      fx_costs_training=get_nmb_est, fx_costs_evaluation=get_nmb,
      intercept_adjustment=-2, return_calibration_plot = T
    )
  
  calibration_experiment_list <- list(
    up2=x_adjusted_up2,
    up1=x_adjusted_up,
    calibrated=x_calibrated,
    down1=x_adjusted_down,
    down2=x_adjusted_down2
  )
  
  saveRDS(calibration_experiment_list, "output/calibration_experiment/calibration_simulations.rds")
}

calibration_experiment_list <- readRDS("output/calibration_experiment/calibration_simulations.rds")

cols_rename <- c(
  "Treat All"="treat_all",
  "Treat None"="treat_none",
  "Cost- effective" ="cost_effective",
  "Closest to (0, 1)"="er", 
  "Youden"="youden", 
  "Sens-Spec product"="cz", 
  "Index of Union"="iu",
  "Cost- Minimising"="cost_minimising"
)


inb_plots <- get_plot_list(
  out_list=calibration_experiment_list, 
  rename_vector=cols_rename, 
  inb_ref_col = "Treat All",
  get_what="inb",
  agg_line_size=1
  # extra_theme=theme(panel.spacing  = unit(0, "lines")) # comment this line out to keep default spacing between facets
)

calibration_plots <- get_plot_list(
  out_list=calibration_experiment_list, 
  get_what="calibration"
)

inb_plots <- keep_only_first_plot_strip(inb_plots)

# test the ggplot below to find the extra rel-height for the first plot (with strips)
# ggsave(
#   plot=cowplot::plot_grid(plotlist=inb_plots, ncol=1, rel_heights = c(0.375, rep(0.3, length(inb_plots)-1))),
#   filename="output/calibration_experiment/inb_plots.jpeg", 
#   height=10.5, width=14)
# inb_cowplot <- cowplot::plot_grid(plotlist=inb_plots, ncol=1)

inb_cowplot <- cowplot::plot_grid(plotlist=inb_plots, ncol=1, rel_heights = c(0.375, rep(0.3, length(inb_plots)-1)))


# cowplot::plot_grid(plotlist=calibration_plots, ncol=1)

calibration_cowplot <- plot_grid(
  plotlist = c(list(NULL), calibration_plots), 
  ncol=1, 
  rel_heights = c(0.075,rep(0.3, length(calibration_plots)))
)

# calibration_cowplot <- plot_grid(plotlist=calibration_plots, ncol=1)
combined_fig <- plot_grid(inb_cowplot, calibration_cowplot, ncol=2, rel_widths = c(0.6,0.25))
ggsave(filename="output/calibration_experiment/calibration_plots.jpeg", plot=combined_fig, dpi=600, height=8, width=7.3)

```

attempts at extracting the strip and adding it to an empty plot
- currently struggling to get the axis out of the original plot to add to new plot
```{r}
get_plot_component(plot, "guide-box")
ggplot() + draw_grob(get_plot_component(p, "panel"))

ggplot() + draw_grob(get_plot_component(p, "panel-1"))



p <- 
  calibration_experiment_list[[1]]$df_result %>% 
  rename(any_of(cols_rename)) %>%
  # select(1:2) %>%
  plot_fw_histogram()


gg_fw_build <-
  ggplot_build(p)$data[[1]] %>%
  mutate(bindwidth=xmax-xmin)

mean(gg_fw_build$bindwidth)
layer_scales(p)$y$range$range
layer_scales(p)$x$range$range


theme_leftmost <- theme(
  # axis.title.y = element_blank(),
  # axis.text.y = element_blank(),
  # axis.ticks.y = element_blank(),
  
  axis.title.x = element_blank(),
  axis.text.x = element_blank(),
  axis.ticks.x = element_blank(),
  
  plot.margin = margin(5.5, 0, 0, 5.5)
)

theme_others <- theme(
  axis.title.y = element_blank(),
  axis.text.y = element_blank(),
  axis.ticks.y = element_blank(),
  
  axis.title.x = element_blank(),
  axis.text.x = element_blank(),
  axis.ticks.x = element_blank(),
  plot.margin = margin(5.5, 0, 0, 0)
)

cols_rename <- c(
  "Treat All (reference)"="treat_all",
  "Treat None"="treat_none",
  "Cost-effective (proposed)" ="cost_effective",
  "Closest to (0, 1)"="er", 
  "Youden Index"="youden", 
  "Sens-Spec product"="cz", 
  "Index of Union"="iu",
  "Cost- Minimising"="cost_minimising"
)

for(i in 1:length(cols_rename)){
  newplot <- calibration_experiment_list[[1]]$df_result %>% 
  rename(any_of(cols_rename)) %>%
  select(1, names(cols_rename)[i]) %>%
  plot_fw_histogram(
    binwidth=mean(gg_fw_build$bindwidth)
  ) +
  scale_y_continuous(limits=layer_scales(p)$y$range$range)
  
  if(i == 1){
    newplot <- newplot + theme_leftmost
    plotlist_out <- list(newplot)
  } else {
    newplot <- newplot + 
      scale_x_continuous(limits=layer_scales(p)$x$range$range) +
      theme_others
    plotlist_out <- c(plotlist_out, list(newplot))
  }
}

plot_grid(plotlist=plotlist_out, nrow=1, rel_widths = c(0.5, rep(0.3, length(plotlist_out)-1)))

p1 <- calibration_experiment_list[[1]]$df_result %>% 
  rename(any_of(cols_rename)) %>%
  select(1, 2) %>%
  plot_fw_histogram(
    binwidth=mean(gg_fw_build$bindwidth)
  ) +
  scale_x_continuous(limits=layer_scales(p)$x$range$range) +
  scale_y_continuous(limits=layer_scales(p)$y$range$range)

p2 <- calibration_experiment_list[[1]]$df_result %>% 
  rename(any_of(cols_rename)) %>%
  select(1, 3) %>%
  plot_fw_histogram(
    binwidth=mean(gg_fw_build$bindwidth)
  ) +
  scale_x_continuous(limits=layer_scales(p)$x$range$range) +
  scale_y_continuous(limits=layer_scales(p)$y$range$range)

p3 <- calibration_experiment_list[[1]]$df_result %>% 
  rename(any_of(cols_rename)) %>%
  select(1, 4) %>%
  plot_fw_histogram(
    binwidth=mean(gg_fw_build$bindwidth)
  ) +
  scale_x_continuous(limits=layer_scales(p)$x$range$range) +
  scale_y_continuous(limits=layer_scales(p)$y$range$range)


plot_grid(plotlist=(list(p1, p2, p3)), nrow=1)
```

# Primary analyses

```{r, fig.height=10, fid.width=7}
if (do_new_analyses | !file.exists("output/primary_analyses/falls_primary_analyses.rds")) {
  falls_simulation <- 
    do_simulation(
      n_sims=5000, n_valid=10000, sim_auc=0.7, event_rate=0.036,
      fx_costs_training=get_nmb_est, fx_costs_evaluation=get_nmb
    )
  saveRDS(falls_simulation, "output/primary_analyses/falls_primary_analyses.rds")
}
falls_simulation <- readRDS("output/primary_analyses/falls_primary_analyses.rds")

if (do_new_analyses | !file.exists("output/primary_analyses/falls_primary_analyses.rds")) {
  icu_simulation <- 
    do_simulation(
      n_sims=5000, n_valid=10000, sim_auc=0.7, event_rate=0.01,
      fx_costs_training=get_nmb_est_ICU, fx_costs_evaluation=get_nmb_ICU
    )
  saveRDS(icu_simulation, "output/primary_analyses/icu_primary_analyses.rds")
}
icu_simulation <- readRDS("output/primary_analyses/icu_primary_analyses.rds")
```

## Plots with most realistic AUC and event rate for each use-case

```{r}
cols_rename <- c(
  "Treat All"="treat_all",
  "Treat None"="treat_none",
  "Cost- effective" ="cost_effective",
  "Closest to (0, 1)"="er", 
  "Youden"="youden", 
  "Sens-Spec product"="cz", 
  "Index of Union"="iu",
  "Cost- Minimising"="cost_minimising"
)

PA_CFG <- list(agg_line_size=1)

p_falls_nmb <-
  falls_simulation$df_result %>%
  # select(-cost_minimising) %>%
  rename(any_of(cols_rename)) %>%
  plot_fw_histogram(
    hdi=F, 
    plot_labels=labs(x="Incremental Net Monetary Benefit (AUD)\n", y=""), 
    inb_ref_col='Treat All',
    agg_line_size=PA_CFG$agg_line_size
  )



p_falls_cutpoints <- 
  falls_simulation$df_thresholds %>%
  select(-treat_all, -treat_none) %>%
  # select(-cost_minimising) %>%
  rename(., any_of(cols_rename)) %>%
  plot_fw_histogram(
    hdi=F, 
    plot_labels=labs(x="Selected cutpoint\n", y=""), 
    dollar_format=F,
    agg_line_size=PA_CFG$agg_line_size
  ) 



p_icu_nmb <-
  icu_simulation$df_result %>%
  rename(any_of(cols_rename)) %>%
  plot_fw_histogram(
    hdi=F,
    plot_labels=labs(x="Incremental Net Monetary Benefit (AUD)\n", y=""),
    inb_ref_col='Treat None',
    agg_line_size=PA_CFG$agg_line_size
  )

p_icu_cutpoints <-
  icu_simulation$df_thresholds %>%
  select(-treat_all, -treat_none) %>%
  rename(., any_of(cols_rename)) %>%
  plot_fw_histogram(
    hdi=F,
    plot_labels=labs(x="Selected cutpoint\n", y=""),
    dollar_format=F,
    agg_line_size=PA_CFG$agg_line_size
  )


plot_grid(
  plotlist=list(
    p_icu_nmb, p_falls_nmb,
    p_icu_cutpoints, p_falls_cutpoints),
  labels=LETTERS[1:4],
  ncol=2
)
# ggsave(filename="output/primary_analyses/primary_results.jpeg", dpi=600, height=7, width=11)


# cowplot::plot_grid(
#   plotlist=list(p_falls_nmb,p_falls_cutpoints),
#   labels=LETTERS[1:4],
#   ncol=2
# )
# ggsave(filename="output/primary_analyses/primary_results_falls-only_for-deliverables.jpeg", dpi=600, height=4.5, width=11)

```


## Table with most realistic AUC and event rate for each use-case
```{r}
falls_summary <- do.call(
  get_summary,
  c(
    falls_simulation$meta_data, 
    list(
      data=falls_simulation$df_result, agg_fx=median, hdi=F, ci=0.95, 
      recode_methods_vector=cols_rename, inb_ref_col="treat_all"
    )
  )
)

icu_summary <- do.call(
  get_summary,
  c(
    icu_simulation$meta_data, 
    list(
      data=icu_simulation$df_result, agg_fx=median, hdi=F, ci=0.95, 
      recode_methods_vector=cols_rename, inb_ref_col="treat_none"
    )
  )
)

primary_analyses_table <- 
  rbind(icu_summary, falls_summary) %>%
  select(
    "Cutpoint method"=method, 
    "Incremental Net Monetary Benefit (AUD)"=summary, 
    "Best performing [n (%)]"=n_best_percent
  ) %>%
  formattable() %>%
    kable(
      escape=F,
      caption=glue::glue("Incremental Net Monetary Benefit presented as median [{percent(0.95, digits=0)} Intervals]")
    ) %>%
  pack_rows(
    index=c(
      "ICU readmission (reference group: Treat None)"=nrow(icu_summary), 
      "Inpatient falls (reference group: Treat All)"=nrow(falls_summary)
    )) %>%
  kable_styling()

# primary_analyses_table
primary_analyses_table %>% save_kable(file="output/primary_analyses/primary_analyses.html")



# primary analyses for deliverable (falls only and remove cost-minimising cutpoint method)
do.call(
  get_summary,
  c(
    falls_simulation$meta_data, 
    list(
      data=select(falls_simulation$df_result, -cost_minimising), agg_fx=median, hdi=F, ci=0.95, 
      recode_methods_vector=cols_rename, inb_ref_col="treat_all"
    )
  )
) %>%
  select(
    "Cutpoint method"=method, 
    "Incremental Net Monetary Benefit (AUD)"=summary, 
    "Best performing [n (%)]"=n_best_percent
  ) %>%
  formattable() %>%
    kable(
      escape=F,
      caption=glue::glue("Incremental Net Monetary Benefit presented as median [{percent(0.95, digits=0)} Intervals]")
    ) %>%
  kable_styling() %>%
  save_kable(file="output/primary_analyses/primary_results_falls-only_for-deliverables.html")


```

## search through a grid of combinations of AUC and event rates to see how this influences the differences between probability threshold methods. The same costs were used in all simulations (distributions at top of document) and are resampled separately for training and validation.
#### In the plot below, the columns, from left to right, have increasing AUC. The rows, from top to bottom, have increasing event rates.

### My hot takes:
#### Probability threshold selection method becomes increasingly important as the AUC of the model and the event rate reduces.
#### I think that this is because, for models with very high discrimination, they're able to correctly classify a larger proportion of samples, and there are fewer which are classified differently based on selection method. The difference is greater for smaller event rates because a false negative is the most costly classification, and only the cost-based method is "aware" of this. This is also why, when the event rate is very high, there is not much of a difference between methods (cost-based method is focused on correctly classifying the majority class but so are the other methods).
```{r, fig.height=24, fig.width=12}
simulation_config <- list(
  n_sims = 5000,
  n_valid = 10000
)

  g_falls <- expand.grid(
    sim_auc=c(0.55, 0.7, 0.85),
    event_rate=c(0.01, 0.036, 0.1)
    # sim_auc=c(0.7),
    # event_rate=seq(0.01, 0.075, 0.005)
  )
  
  g_icu <- expand.grid(
    sim_auc=c(0.55, 0.7, 0.85),
    event_rate=c(0.01, 0.025, 0.1)
  )

if (do_new_analyses | !file.exists("output/sensitivity_analyses/falls_sensitivity_analyses.rds")) {
  n_cluster <- detectCores() - 2
  cl <- makeCluster(n_cluster)
  cl <- parallelly::autoStopCluster(cl)
  
  clusterExport(cl, {
    c("do_simulation", "g_falls", "g_icu", "get_nmb", "get_nmb_est", "get_nmb_ICU", "get_nmb_est_ICU", "params", "simulation_config")
  })
  
  invisible(clusterEvalQ(cl, {
    library(tidyverse)
    library(data.table)
    library(ggridges)
    library(bayestestR)
    library(cutpointr)
    source("src/utils.R")
    source("src/cutpoint_methods.R")
    source("src/summary.R")
  }))
  
  
  ll_falls <- parLapply(
    cl,
    1:nrow(g_falls),
    function(i) do.call(
      do_simulation, 
      c(
        simulation_config, 
        list(
          sim_auc=g_falls$sim_auc[i],
          event_rate=g_falls$event_rate[i],
          fx_costs_training=get_nmb_est, 
          fx_costs_evaluation=get_nmb
        )
      )
    )
  )
  
  saveRDS(ll_falls, "output/sensitivity_analyses/falls_sensitivity_analyses.rds")
}
  
if (do_new_analyses | !file.exists("output/sensitivity_analyses/falls_sensitivity_analyses.rds")) {
  n_cluster <- detectCores() - 2
  cl <- makeCluster(n_cluster)
  cl <- parallelly::autoStopCluster(cl)
  
  clusterExport(cl, {
    c("do_simulation", "g_falls", "g_icu", "get_nmb", "get_nmb_est", "get_nmb_ICU", "get_nmb_est_ICU", "params", "simulation_config")
  })
  
  invisible(clusterEvalQ(cl, {
    library(tidyverse)
    library(data.table)
    library(ggridges)
    library(bayestestR)
    library(cutpointr)
    source("src/utils.R")
    source("src/cutpoint_methods.R")
    source("src/summary.R")
  }))
  
  ll_icu <- parLapply(
    cl,
    1:nrow(g_icu),
    function(i) do.call(
      do_simulation,
      c(
        simulation_config,
        list(
          sim_auc=g_icu$sim_auc[i],
          event_rate=g_icu$event_rate[i],
          fx_costs_training=get_nmb_est_ICU,
          fx_costs_evaluation=get_nmb_ICU
        )
      )
    )
  )
  
  saveRDS(ll_icu, "output/sensitivity_analyses/icu_sensitivity_analyses.rds")
}


# save specific screen for falls across event rates
# saveRDS(ll_falls, "output/sensitivity_analyses/falls_sensitivity_analyses2.rds")

# load simulation results
ll_falls <- readRDS("output/sensitivity_analyses/falls_sensitivity_analyses.rds")
ll_icu <- readRDS("output/sensitivity_analyses/icu_sensitivity_analyses.rds")
```

```{r}

falls_inb_plots <- get_plot_list(
  ll_falls, cols_rename, get_what="inb", 
  inb_ref_col="Treat All", groups_remove="Cost- Minimising"
)

falls_cp_plots <- get_plot_list(ll_falls, cols_rename, get_what="cutpoints")

plot_grid(
  plotlist=c(falls_inb_plots, falls_cp_plots),
  nrow=2,
  labels=rep(g_falls$event_rate, 2)
)

ggsave(filename="output/sensitivity_analyses/falls_simulations_inb_for-deliverables_sanity-check.jpeg", height=10, width=45)
```



# save plots from sensitivity analyses
```{r}
# FALLS
falls_inb_plots <- get_plot_list(
  ll_falls, cols_rename, get_what="inb", inb_ref_col="Treat All", 
  groups_remove="Cost- Minimising", only_show_interval=T
)

# add labels to left most graphs for event rate
event_rate_str <- function(rate) glue::glue("Event Rate = {rate}")
model_auc_str <- function(auc) glue::glue("Model AUC = {auc}")

falls_inb_plots[[1]] <- falls_inb_plots[[1]] + xlab(event_rate_str(g_falls$event_rate[1]))
falls_inb_plots[[4]] <- falls_inb_plots[[4]] + xlab(event_rate_str(g_falls$event_rate[4]))
falls_inb_plots[[7]] <- falls_inb_plots[[7]] + xlab(event_rate_str(g_falls$event_rate[7]))

falls_inb_plots[[7]] <- falls_inb_plots[[7]] + ylab(model_auc_str(g_falls$sim_auc[7]))
falls_inb_plots[[8]] <- falls_inb_plots[[8]] + ylab(model_auc_str(g_falls$sim_auc[8]))
falls_inb_plots[[9]] <- falls_inb_plots[[9]] + ylab(model_auc_str(g_falls$sim_auc[9]))

for(i in 1:length(falls_inb_plots)){
  falls_inb_plots[[i]] <- falls_inb_plots[[i]] + 
    theme(
      plot.margin = unit(c(0, 0, 0, 0), "cm"),
      # panel.grid.major.x = element_blank(),
      # panel.grid.minor.x = element_blank()
    )
}

# add border to primary analyses within panels
falls_inb_plots[[5]] <- 
  falls_inb_plots[[5]] +
  theme(
    strip.background = element_rect(fill="#ff8ba0")
  )

plot_grid(
  plotlist=falls_inb_plots,
  ncol=length(unique(g_falls$sim_auc))
)

ggsave(filename="output/sensitivity_analyses/falls_simulations_inb_for-deliverables.jpeg", height=24*0.5, width=15)
# ggsave(filename="output/sensitivity_analyses/falls_simulations_inb.jpeg", height=24*0.5, width=15)



# ICU discharge
icu_inb_plots <- get_plot_list(ll_icu, cols_rename, get_what="inb", inb_ref_col="Treat None")

icu_inb_plots[[1]] <- icu_inb_plots[[1]] + xlab(event_rate_str(g_icu$event_rate[1]))
icu_inb_plots[[4]] <- icu_inb_plots[[4]] + xlab(event_rate_str(g_icu$event_rate[4]))
icu_inb_plots[[7]] <- icu_inb_plots[[7]] + xlab(event_rate_str(g_icu$event_rate[7]))

icu_inb_plots[[7]] <- icu_inb_plots[[7]] + ylab(model_auc_str(g_icu$sim_auc[7]))
icu_inb_plots[[8]] <- icu_inb_plots[[8]] + ylab(model_auc_str(g_icu$sim_auc[8]))
icu_inb_plots[[9]] <- icu_inb_plots[[9]] + ylab(model_auc_str(g_icu$sim_auc[9]))

for(i in 1:length(icu_inb_plots)){
  icu_inb_plots[[i]] <- icu_inb_plots[[i]] + 
    theme(
      plot.margin = unit(c(0, 0, 0, 0), "cm"),
      # panel.grid.major.x = element_blank(),
      # panel.grid.minor.x = element_blank()
    )
}

# add border to primary analyses within panels
icu_inb_plots[[5]] <- 
  icu_inb_plots[[5]] +
  theme(
    strip.background = element_rect(fill="#ff8ba0")
  )


plot_grid(
  plotlist=icu_inb_plots,
  ncol=length(unique(g_icu$sim_auc))
)
ggsave(filename="output/sensitivity_analyses/icu_simulations_inb.jpeg", height=24*0.5, width=15)
```

```{r}
# save selected cutpoints plots
# cowplot::plot_grid(
#   plotlist=get_plot_list(ll_falls, cols_rename, get_what="cutpoints"), 
#   ncol=length(unique(g_falls$sim_auc))
# )
# ggsave(filename="output/falls_simulations_thresholds.jpeg", height=24, width=12)
# 
# cowplot::plot_grid(
#   plotlist=get_plot_list(ll_icu, cols_rename, get_what="cutpoints"),
#   ncol=length(unique(g_icu$sim_auc))
# )
# ggsave(filename="output/icu_simulations_thresholds.jpeg", height=24, width=12)
```


```{r}
make_table(
  ll_falls, get_what="inb", rename_vector=cols_rename, 
  save_path="output/sensitivity_analyses/falls_inb_summary.html",
  inb_ref_col="Treat All"
)

make_table(
  ll_falls, get_what="cutpoints", rename_vector=cols_rename, 
  save_path="output/sensitivity_analyses/falls_thresholds_summary.html"
)

make_table(
  ll_icu, get_what="inb", rename_vector=cols_rename, 
  save_path="output/sensitivity_analyses/icu_inb_summary.html", 
  inb_ref_col="Treat None"
)

make_table(
  ll_icu, get_what="cutpoints", rename_vector=cols_rename, 
  save_path="output/sensitivity_analyses/icu_thresholds_summary.html"
)

```



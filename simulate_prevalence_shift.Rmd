---
title: "simulate_prevalence_shift"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: rmarkdown::github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
source("src/utils.R")
```

```{r}
set.seed(123)
# N = samples
N <- 1000 
# np = number of fixed effects, excluding the intercept
np <- 6

## fixed effects
x0 <- rnorm(1)
x1 <- rnorm(1)
x2 <- rnorm(1)
x3 <- rnorm(1)
x4 <- rnorm(1)
x5 <- 0
x6 <- 0

B <- matrix(c(x0, x1, x2, x3, x4, x5, x6), ncol=1)

X <- cbind(rep(1, N), matrix(rnorm(n=np*N), ncol=np))

pi <- X %*% B + rnorm(n=N)
p <- exp(pi)/(1+exp(pi))

out <- rbinom(n=N, 1, p)
glue::glue("Prevalence in the training set: {mean(out)}")

rocobj <- pROC::roc(as.factor(out), p)
glue::glue("AUC in training set: {round(rocobj$auc, 4)}")

## check to see if predictions are calibrated
data.frame(prob=p, class=out) %>%
  ggplot(aes(prob, out)) + 
  geom_smooth() + 
  geom_abline() +
  labs(title="Model is well calibrated on training data")

cost_vector <- c("TN"=0, "FN"=100, "TP"=80, "FP"=5)
train_thresholds <- get_thresholds(predicted=p, actual=out, costs=cost_vector)
train_thresholds
```


# sample new dataset with shift in distribution of fixed effects to cause change in prevalence of event
```{r}
set.seed(123)
# shift the mean of the observed predictors
X <- cbind(rep(1, N), matrix(rnorm(n=np*N, mean=1), ncol=np))
pi <- X %*% B + rnorm(n=N)
p <- exp(pi)/(1+exp(pi))

out <- rbinom(n=N, 1, p)
glue::glue("Prevalence in the prevalence-shifted validation set: {mean(out)}")

rocobj <- pROC::roc(as.factor(out), p)
glue::glue("AUC in prevalence-shifted validation set: {round(rocobj$auc, 4)}")

# model is still calibrated in prevalence-shifted validation set
valid_df <- data.frame(pred=p, actual=out) 

valid_df %>%
  ggplot(aes(pred, actual)) + 
  geom_smooth() + 
  geom_abline() +
  labs(title="Model is well calibrated on validation data with shifted prevalence of outcome")

```

## There's a big shift in the probabilty threshold selected by youden, but not by the cost-effective method

### Suggests that probability thresholds selected by cost-effectiveness are more generalisable to new populations where the prevalence may have shifted but costs of interventions/QALYs hasn't
```{r}
cost_vector <- c("TN"=0, "FN"=100, "TP"=80, "FP"=5)
get_thresholds(predicted=valid_df$pred, actual=valid_df$actual, costs=cost_vector)
```


## If the Pt was selected based on costs vs youden from original model development, how would it affect costs on validation data with shift in prevalence?
```{r}
valid_ce_cost <- classify_samples(
  predicted=valid_df$pred, 
  actual=valid_df$actual, 
  pt=train_thresholds$cost_effective, 
  costs=cost_vector
)
# costs per patient of using cost-effective selection of Pt
valid_ce_cost

valid_youden_cost <- classify_samples(
  predicted=valid_df$pred, 
  actual=valid_df$actual, 
  pt=train_thresholds$youden,
  costs=cost_vector
)
# costs per patient of using Youden index for selection of Pt
valid_youden_cost
```


## If there is a shift in the costs associated with the (mis)classification, is it beneficial to re-select Pt
#### (No shift in prevalence)
```{r}
set.seed(1234)
# use a larger sample - same sample size to select threshold with new costs, evaluating on the remainder
N <- 1e4
# shift the mean of the observed predictors
X <- cbind(rep(1, N), matrix(rnorm(n=np*N), ncol=np))
pi <- X %*% B + rnorm(n=N)
p <- exp(pi)/(1+exp(pi))

out <- rbinom(n=N, 1, p)
glue::glue("Prevalence in the prevalence-shifted validation set: {mean(out)}")

rocobj <- pROC::roc(as.factor(out), p)
glue::glue("AUC in prevalence-shifted validation set: {round(rocobj$auc, 4)}")

# model is still calibrated in prevalence-shifted validation set
df_all <- data.frame(pred=p, actual=out)
df_train <- df_all[1:1000,]
df_valid <- df_all[1001:N,]

# decrease in increase in FP - represents an increase in the cost of the intervention?
# cost_vector <- c("TN"=0, "FN"=100, "TP"=80, "FP"=5)
new_cost_vector <- c("TN"=0, "FN"=100, "TP"=125, "FP"=50) 
new_thresholds <- get_thresholds(predicted=df_train$pred, actual=df_train$actual, c=new_cost_vector)
# new_thresholds$cost_effective
# train_thresholds$cost_effective
ce_cost_refit <- classify_samples(
  predicted=df_valid$pred, 
  actual=df_valid$actual, 
  pt=new_thresholds$cost_effective, 
  costs=new_cost_vector
)
# costs per patient of using a refit Pt by cost-effectiveness
ce_cost_refit

ce_cost_orig <- classify_samples(
  predicted=df_valid$pred, 
  actual=df_valid$actual, 
  pt=train_thresholds$cost_effective,
  costs=new_cost_vector
)

# costs per patient of using a the original Pt by cost-effectiveness
ce_cost_orig
```

### The benefit of re-estimating Pt based on cost-effectiveness will depend on how large the change in costs are, and whether there is a change in the prevalence of the event.


# Assess new approach where the prevalence is controlled by a change in the underlying distribution of the covariates
### still needs to move to sampling from mvrnorm rather than assuming there is no covariance in the predictors
```{r}

set.seed(123)
# N = samples
N <- 100000
# np = number of fixed effects, excluding the intercept
np <- 6

# desired sample size 
n = 1000

# desired prevalence of sample
r <- 0.1

## fixed effects
x0 <- -4.8
x1 <- -0.5
x2 <- -0.2
x3 <- -0.1
x4 <- -3
x5 <- 0
x6 <- 0

B <- matrix(c(x0, x1, x2, x3, x4, x5, x6), ncol=1)

X <- cbind(rep(1, N), matrix(rnorm(n=np*N, mean=0), ncol=np))

pi <- X %*% B + rnorm(n=N, mean=0, sd=4)
p <- exp(pi)/(1+exp(pi))

out <- rbinom(n=N, 1, p)

expected_prevalence <- mean(p)
observed_prevalence <- mean(out)
observed_prevalence

df_train <- as.data.frame.matrix(cbind(out, X))
names(df_train) <- c("event", paste0("X",0:6))
df_train$X0 <- NULL

m <- glm(event~., data=df_train, family=binomial(link="logit"))

fitted <- predict(m, type="response")

do_cv <- function(data, nfold=5){
  data$fold <- sample(rep(1:nfold, length.out=nrow(data)), replace=F)
  aucs <- c()
  for(f in 1:nfold){
    train <- data[data$fold==f,]
    valid <- data[data$fold!=f,]
    
    train$fold <- NULL
    valid$fold <- NULL
    
    model <- glm(event~., data=train, family=binomial(link="logit"))
    
    preds <- predict(model, type="response", newdata=valid)
    aucs <- append(aucs, get_auc(predicted=preds, actual=valid$event))
  }
  return(mean(aucs))
}
do_cv(df_train)

full_model <- glm(event~., data=df_train, family=binomial(link="logit"))
```

```{r}
set.seed(42)
X <- cbind(rep(1, N), matrix(rnorm(n=np*N, mean=0.5, sd=0.9), ncol=np))

pi <- X %*% B + rnorm(n=N, mean=0, sd=4)
p <- exp(pi)/(1+exp(pi))

out <- rbinom(n=N, 1, p)

expected_prevalence <- mean(p)
observed_prevalence <- mean(out)
observed_prevalence

df_valid <- as.data.frame.matrix(cbind(out, X))
names(df_valid) <- c("event", paste0("X",0:6))
df_valid$X0 <- NULL

valid_preds <- predict(full_model, type="response", newdata=df_valid)

get_auc(predicted=valid_preds, actual=df_valid$event)

```


```{r}

data.frame(fitted=valid_preds, actual=df_valid$event) %>%
  head(2000) %>%
  ggplot(aes(fitted, actual)) + 
  geom_smooth(method="loess") + 
  geom_abline() +
  labs(title="calibration looks fine in the validation data with a shifted prevalence")

```


# Basing threshold on maximising NMB is robust to a shift in prevalence while costs and AUC model descrimination remain constant. 
### I think that this is a realistic situation where a model is generalised to a new context where patient population is different but the costs of interventions/QALYs may not be.
```{r}
cost_vector <- c("TN"=0, "FN"=100, "TP"=80, "FP"=20)
cat("Thresholds selected if based on youden and NMB on training data for model")
get_thresholds(predicted=fitted, actual=df_train$event, costs=cost_vector)

cat("Thresholds selected if based on youden and NMB on 'external' validation data")
get_thresholds(predicted=valid_preds, actual=df_valid$event, costs=cost_vector)
```
